{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"DM_HW5_20141591.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_zxIyr_rjTMh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593049252837,"user_tz":-540,"elapsed":3486,"user":{"displayName":"최기용","photoUrl":"","userId":"08701122335805371632"}}},"source":["import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PjFKJ83jTMy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593049252840,"user_tz":-540,"elapsed":3479,"user":{"displayName":"최기용","photoUrl":"","userId":"08701122335805371632"}}},"source":["class DDPG_Mu(nn.Module):\n","    def __init__(self):\n","        super(DDPG_Mu, self).__init__()\n","        self.fc1 = nn.Linear(3, 512)\n","        self.fc_mu = nn.Linear(512, 1)\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        mu = torch.tanh(self.fc_mu(x))*2\n","        return mu\n","    \n","    def train(self, loss):\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","class DDPG_Q(nn.Module):\n","    def __init__(self):\n","        super(DDPG_Q, self).__init__()\n","        self.fc_a = nn.Linear(1, 64)\n","        self.fc_s = nn.Linear(3, 64)\n","        self.fc_1 = nn.Linear(128, 128)\n","        self.fc_q = nn.Linear(128, 1)\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n","    \n","    def forward(self, x, a):\n","        x1 = F.relu(self.fc_a(a))\n","        x2 = F.relu(self.fc_s(x))\n","        x = torch.cat([x1, x2], dim=1)\n","        x = F.relu(self.fc_1(x))\n","        q = self.fc_q(x)\n","        return q\n","\n","    \n","    def train(self, loss):\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdt8Io-KjTM_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593049252841,"user_tz":-540,"elapsed":3478,"user":{"displayName":"최기용","photoUrl":"","userId":"08701122335805371632"}}},"source":["env = gym.make('Pendulum-v0')\n","Q, Q_p, Mu, Mu_p = DDPG_Q(), DDPG_Q(), DDPG_Mu(), DDPG_Mu()\n","GAMMA = 0.99 #discount factor\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 30000 #replay buffer size\n","replay_buffer = [] #다른 자료구조로 바꾸어도 상관없음.(list, queue, dict 등)\n","TAU = 0.01 #soft update parameter\n","PARAMETER_NOISE_COEF = 0.0005\n","ITER = 10 #training 함수가 호출될때 학습 iteration 횟수."],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kJHY0I2jTNJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593049253446,"user_tz":-540,"elapsed":4080,"user":{"displayName":"최기용","photoUrl":"","userId":"08701122335805371632"}}},"source":["def training():\n","    ###############Put your code here############\n","    for i in range(ITER):\n","      minibatch = make_minibatch()\n","      state_b = []\n","      action_b = []\n","      reward_b = []\n","      next_state_b =[]\n","      done_b = []\n","      for state, action, reward, next_state, done in minibatch:\n","        state_b.append(state)\n","        action_b.append(action)\n","        reward_b.append(reward)\n","        next_state_b.append(next_state)\n","        done_b.append(done)\n","      for i in range(len(state_b)):    \n","        state_b[i] = state_b[i].tolist()\n","      state_b = np.array(state_b).reshape(32,3)\n","      action_b = np.array(action_b).reshape(32,1)\n","      reward_b =np.array(reward_b).reshape(32,1)\n","      for i in range(len(next_state_b)):    \n","        next_state_b[i] = next_state_b[i].tolist()\n","      next_state_b = np.array(next_state_b).reshape(32,3)\n","      done_b = np.array(done_b,dtype=bool).reshape(32,1)\n","      state_tensor = torch.from_numpy(state_b).float()\n","      action_tensor = torch.from_numpy(action_b).float()\n","      reward_tensor = torch.from_numpy(reward_b).float()\n","      next_state_tensor = torch.from_numpy(next_state_b).float()\n","      done_tensor = torch.from_numpy(done_b)\n","\n","      y = reward_tensor + GAMMA* Q_p(next_state_tensor,Mu_p(next_state_tensor))\n","      critic_l = F.mse_loss(Q(state_tensor,action_tensor),y)\n","      Q.train(critic_l)\n","      actor_l = -Q(state_tensor,Mu(state_tensor)).mean()\n","      Mu.train(actor_l)\n","\n","      soft_target_update(Mu,Mu_p)\n","      soft_target_update(Q,Q_p)\n","    #############################################\n","    \n","def soft_target_update(model, model_p):\n","    ###############Put your code here############\n","    # print(model)\n","    for param,target in zip(model.parameters(), model_p.parameters()):\n","      target.data = target.data*(1-TAU) + param.data*TAU\n","    #############################################\n","        \n","def init_target_param(model, model_p):\n","    ###############Put your code here############\n","    for param, target in zip(model.parameters(), model_p.parameters()):\n","      target.data = param.data\n","    #############################################\n","        \n","def parameter_noise(model):\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param.add_(torch.randn(param.size()) * PARAMETER_NOISE_COEF)\n","            \n","def store_transition(s, a, r, s_prime, done):\n","    ###############Put your code here############\n","    tmp = [s,a,r,s_prime,done]\n","    if len(replay_buffer) >= BUFFER_SIZE :\n","        replay_buffer.pop(0)\n","    replay_buffer.append(tmp)\n","    #############################################\n","    \n","def make_minibatch():\n","    ###############Put your code here############\n","    minibatch = random.sample(replay_buffer,BATCH_SIZE)\n","    \n","    return minibatch\n","    #############################################"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rTpxcI81jTNX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593049460338,"user_tz":-540,"elapsed":210965,"user":{"displayName":"최기용","photoUrl":"","userId":"08701122335805371632"}},"outputId":"e0c60323-1cdc-4498-b717-143f91b02ae6"},"source":["reward_sum = 0.0\n","reward_list = []\n","init_target_param(Mu, Mu_p)\n","init_target_param(Q, Q_p)\n","\n","for ep in range(20000):\n","    observation = env.reset()\n","    while True:\n","        state = torch.tensor(observation, dtype=torch.float)\n","        parameter_noise(Mu)\n","        action = Mu(state).detach()\n","        observation, reward, done, _ = env.step([action.item()])\n","        reward_sum += reward\n","        next_state = torch.tensor(observation, dtype=torch.float)\n","        store_transition(state, action, reward, next_state, done)\n","        if done:\n","            break\n","            \n","    if len(replay_buffer) >= 500:\n","        training()\n","            \n","    if ep % 20 == 19:\n","        print('Episode %d'%ep,', Reward mean : %f'%(reward_sum/20.0))\n","        if reward_sum/20.0 > -200.0:\n","            break\n","        reward_sum = 0.0"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Episode 19 , Reward mean : -1350.407593\n","Episode 39 , Reward mean : -1243.094730\n","Episode 59 , Reward mean : -1236.100445\n","Episode 79 , Reward mean : -1491.276838\n","Episode 99 , Reward mean : -1766.659902\n","Episode 119 , Reward mean : -1586.728410\n","Episode 139 , Reward mean : -1508.894156\n","Episode 159 , Reward mean : -1440.508896\n","Episode 179 , Reward mean : -1274.340279\n","Episode 199 , Reward mean : -1288.026364\n","Episode 219 , Reward mean : -1160.041389\n","Episode 239 , Reward mean : -1177.602819\n","Episode 259 , Reward mean : -1098.795469\n","Episode 279 , Reward mean : -1220.525328\n","Episode 299 , Reward mean : -1226.366781\n","Episode 319 , Reward mean : -1030.278114\n","Episode 339 , Reward mean : -869.598597\n","Episode 359 , Reward mean : -837.045471\n","Episode 379 , Reward mean : -828.768274\n","Episode 399 , Reward mean : -815.158301\n","Episode 419 , Reward mean : -865.327387\n","Episode 439 , Reward mean : -731.700358\n","Episode 459 , Reward mean : -786.530504\n","Episode 479 , Reward mean : -912.133507\n","Episode 499 , Reward mean : -912.761088\n","Episode 519 , Reward mean : -904.993226\n","Episode 539 , Reward mean : -782.647482\n","Episode 559 , Reward mean : -715.599831\n","Episode 579 , Reward mean : -626.006340\n","Episode 599 , Reward mean : -984.110035\n","Episode 619 , Reward mean : -1023.014857\n","Episode 639 , Reward mean : -1069.883274\n","Episode 659 , Reward mean : -683.688551\n","Episode 679 , Reward mean : -662.542299\n","Episode 699 , Reward mean : -673.439675\n","Episode 719 , Reward mean : -622.934376\n","Episode 739 , Reward mean : -477.800043\n","Episode 759 , Reward mean : -717.500446\n","Episode 779 , Reward mean : -697.413128\n","Episode 799 , Reward mean : -943.594601\n","Episode 819 , Reward mean : -943.099428\n","Episode 839 , Reward mean : -699.812716\n","Episode 859 , Reward mean : -760.659955\n","Episode 879 , Reward mean : -968.747783\n","Episode 899 , Reward mean : -1150.197999\n","Episode 919 , Reward mean : -887.985784\n","Episode 939 , Reward mean : -1005.694828\n","Episode 959 , Reward mean : -390.041540\n","Episode 979 , Reward mean : -495.687754\n","Episode 999 , Reward mean : -460.945909\n","Episode 1019 , Reward mean : -421.682366\n","Episode 1039 , Reward mean : -281.704410\n","Episode 1059 , Reward mean : -284.645953\n","Episode 1079 , Reward mean : -259.871959\n","Episode 1099 , Reward mean : -232.139331\n","Episode 1119 , Reward mean : -571.858670\n","Episode 1139 , Reward mean : -618.744309\n","Episode 1159 , Reward mean : -829.524775\n","Episode 1179 , Reward mean : -1585.966648\n","Episode 1199 , Reward mean : -1529.416187\n","Episode 1219 , Reward mean : -1516.097134\n","Episode 1239 , Reward mean : -1516.435841\n","Episode 1259 , Reward mean : -1503.408829\n","Episode 1279 , Reward mean : -904.911375\n","Episode 1299 , Reward mean : -706.214862\n","Episode 1319 , Reward mean : -843.446662\n","Episode 1339 , Reward mean : -844.917835\n","Episode 1359 , Reward mean : -749.404905\n","Episode 1379 , Reward mean : -457.317692\n","Episode 1399 , Reward mean : -217.772975\n","Episode 1419 , Reward mean : -256.521921\n","Episode 1439 , Reward mean : -494.315344\n","Episode 1459 , Reward mean : -201.861376\n","Episode 1479 , Reward mean : -212.879246\n","Episode 1499 , Reward mean : -244.360938\n","Episode 1519 , Reward mean : -166.969474\n"],"name":"stdout"}]}]}